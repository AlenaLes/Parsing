{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import ipywidgets as wi\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout\n",
    "import psycopg2                                     # Для подключение к БД\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "HOST    ='10.47.126.122'\n",
    "DB      ='sas'\n",
    "DBUSER  ='lesikhina'\n",
    "DBPASS  ='Xne2J*f~'\n",
    "PORT    ='5432'\n",
    "\n",
    "greenplum = psycopg2.connect(host = HOST, database = DB, user = DBUSER, password = DBPASS)\n",
    "gp_cursor = greenplum.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "   background-color: transparent !important;\n",
       "}\n",
       ".jp-OutputArea-output {\n",
       "   background-color: transparent;\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "   background-color: transparent !important;\n",
    "}\n",
    ".jp-OutputArea-output {\n",
    "   background-color: transparent;\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response:\n",
    "    \"\"\"\n",
    "    Класс для выгрузки данных с сайта, преобразования и загрузки в Dbeaver.\n",
    "\n",
    "    Для проверки подключения использовать - start_respond с аргументом type = test.\n",
    "    Для выгрузки основных категорий использовать start_respond с аргументом type = main.\n",
    "\n",
    "    Для выгрузки остальных данных использовать метод loader:\n",
    "        links           - Набор ссылок, по которым выгружается информация\n",
    "        type            - (category, uncategory(не обязательно), pages, products, name_art)\n",
    "                            Порядок выгрузки - (category, uncategory(не обязательно), pages, products, name_art)\n",
    "                            При выгрузке pages необходимо указать тип ранее используемого уровня page_type - (category, uncategory).\n",
    "        need_to_red     - (True, False) Дефолтно стоит True. Нужно для преобразования файла в нужный формат. При False выдаст сырой список по товарам с артикулами и наименованиями.\n",
    "\n",
    "    Для преобразования данных использовать функцию redactor:\n",
    "        prod_links      - Ссылки продуктов\n",
    "        type            - (category, uncategory). Указать откуда были выгружены ссылки.\n",
    "        want_to_split   - (True, False). Указать True, если хотим разбить df на несколько по категориям. Использовать, если данных очень большое количество.\n",
    "\n",
    "    Для загрузки данных в Dbeaver использовать функцию merge_with_db:\n",
    "        df              - Датафрейм с шага redactor\n",
    "        table_name_for_db Наименование таблицы для загрузки в базу\n",
    "        want_to_create  - (True, False) Дефолтно стоит True. Нужно для загрузки в базу. Если поставить False - загружаться не будет. Выдаст итоговый датафрейм.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.url = 'https://www.auchan.ru'\n",
    "        self.start_time = time.time()\n",
    "        self.style= {'bar_color':'#f08080'}\n",
    "        self.layout = Layout(width='20%', height='20px')\n",
    "\n",
    "    def start_respond(self, type):\n",
    "        response = requests.get(self.url, verify=False)\n",
    "        response.raise_for_status()     \n",
    "        \n",
    "        if type == 'test':\n",
    "            print(response)\n",
    "        \n",
    "        elif type == 'main':\n",
    "            main_links, folder_name = [], []\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            for link in soup.find_all('a','youMayNeedCategoryItem active css-1poaokp'):\n",
    "                main_links.append(self.url+link.get('href'))\n",
    "                folder_name.append(link.get('href').replace('/catalog/',\"\").replace('/',\"\").replace('-',\"_\"))\n",
    "    \n",
    "            return main_links\n",
    "        \n",
    "    def loader(self, links, type, page_type = False, need_to_red = True):\n",
    "        progress = IntProgress(min = 0, max = len(links), style=self.style, layout=self.layout)\n",
    "        display(progress)\n",
    "        num = 0\n",
    "        resp_links, prod_links = [], []\n",
    "\n",
    "        for s_link in links:\n",
    "            try:\n",
    "                response = requests.get(s_link, verify=False)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "                if type == 'category':\n",
    "                    search_attr = 'linkToSubCategory active css-1bxu12v'\n",
    "                    url = self.url\n",
    "                    [resp_links.append(url + link.get('href')) for link in soup.find_all('a', search_attr)]\n",
    "        \n",
    "                elif type == 'uncategory':\n",
    "                    search_attr = 'active css-1g2m3w6 css-zq3cln'\n",
    "                    url = self.url\n",
    "                    [resp_links.append(url + link.get('href')) for link in soup.find_all('a', search_attr)]\n",
    "                \n",
    "                elif type == 'products':\n",
    "                    search_attr = 'productCardPictureLink active css-of3y3a'\n",
    "                    url = s_link               \n",
    "                    [resp_links.append(url + link.get('href')) for link in soup.find_all('a', search_attr)]\n",
    "\n",
    "                elif type == 'pages':\n",
    "\n",
    "                    if page_type == 'category':\n",
    "                        n = soup.find('span','css-105jgwh').text            # Для количества товаров в категории\n",
    "                    elif page_type == 'uncategory':\n",
    "                        n = soup.find('span','css-1vpxcja').text            # Для количества товаров в подкатегории\n",
    "                        \n",
    "                    page = 1\n",
    "                    n = n.replace('(',\"\").replace(')', \"\")\n",
    "                    max_page = math.ceil(int(n)/40)\n",
    "\n",
    "                    while page <= max_page:\n",
    "                        resp_links.append(s_link+'?page={}'.format(page))\n",
    "                        page += 1\n",
    "\n",
    "                elif type == 'name_art':                                            # ДОБАВИТЬ ЗАГРУЗКУ ПО ОТДЕЛЬНОМУ ФАЙЛУ ИЛИ ПОЛНОСТЬЮ\n",
    "                    prod_links.append(s_link + '/'+ soup.find('title').text)        # Если ничего не грузит - проверить значения в строке\n",
    "\n",
    "                    if need_to_red == True:    \n",
    "                        name_art = pd.DataFrame(prod_links)\n",
    "                        name_art.columns=['все']\n",
    "                        name_art = name_art['все'].str.split('/', expand=True)\n",
    "                        name_art.columns=['0','1','2','3','Имя_ENG','5','Имя и арт']\n",
    "                        name_art[['Наименование', 'Артикул']] = name_art['Имя и арт'].map(lambda x: x.lstrip('Купить ').rstrip(') в интернет-магазине АШАН в Москве и России')).str.split('(', expand=True)\n",
    "                        name_art = name_art.drop(columns=['0','1','2','3','5', 'Имя и арт'])\n",
    "                        resp_links = name_art\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                num += 1    \n",
    "                perc = round((num / len(links)) * 100, 2)\n",
    "                end_time = time.time()\n",
    "                predict = round((((end_time - self.start_time)/num)*(len(links)-num))/60,2)\n",
    "                print (' PROCESS: {}/{}  ||  {} %  ||  WAITING TIME {} MIN  ||   '.format(num,len(links),perc,predict), end='\\r')\n",
    "                \n",
    "                output = wi.Output()\n",
    "                progress.value = num\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print ('SOMETHING IS WRONG WITH {}      '.format(s_link), end='\\r')\n",
    "                pass\n",
    "            \n",
    "        return(resp_links)\n",
    "\n",
    "def redactor(prod_links, type, want_to_split):\n",
    "    \n",
    "    prod = pd.DataFrame(prod_links)\n",
    "    prod.to_csv (\"full_path.csv\", index=False)\n",
    "\n",
    "    if type == 'category':\n",
    "        names = ['1','2','3','4','category','uncategory','5','product','name','6']\n",
    "    elif type == 'uncategory':\n",
    "        names = ['1','2','3','4','category','uncategory','subcategory','5','product','name','6']\n",
    "    \n",
    "    path_prod = pd.read_csv('full_path.csv', names=names, encoding='cp1251',sep='/', skiprows=1)\n",
    "    path_prod = path_prod.drop(columns=['1','2','3','4','5','product','6'])\n",
    "    path_prod['path'] = 'https://www.auchan.ru/product/' + path_prod['name'] +'/'\n",
    "\n",
    "    if want_to_split == True:\n",
    "        category = path_prod.category.unique()\n",
    "        files_name = []\n",
    "\n",
    "        for category_name in category:\n",
    "            df = path_prod.loc[(path_prod['category'] == category_name)]\n",
    "            df = df.drop(columns=['category','name'])\n",
    "            file_name = \"{}_part\".format(category_name)\n",
    "            df.to_csv ('{}.csv'.format(file_name), index= False, header= False, sep=';')\n",
    "            files_name.append(file_name)\n",
    "\n",
    "        prodycts_links = []\n",
    "        for i in files_name:\n",
    "            prodycts_links.append(i+'.csv')\n",
    "        print('Всего категорий: {}. Созданы файлы с именами категорий: {}'.format(len(path_prod.category.unique()), files_name))\n",
    "\n",
    "    else:\n",
    "        file_name = \"Full_data\"\n",
    "        path_prod.to_csv ('{}.csv'.format(file_name), index= False, header= False, sep=';')\n",
    "        print('Данные загружены одним файлом. Создан файл с именем: {}'.format(file_name))\n",
    "\n",
    "    return(path_prod)\n",
    "\n",
    "def merge_with_db(df, table_name_for_db = 'lay_test', want_to_create = True):\n",
    "    query_prod = \"\"\"select sku, lvl_1, lvl_1_name, lvl_2, lvl_2_name, lvl_3, lvl_3_name, lvl_4, lvl_4_name from ods_mdm.dim_products\"\"\"\n",
    "    products  =  pd.read_sql(query_prod, greenplum)\n",
    "    products['sku'] = products['sku'].astype(str)\n",
    "\n",
    "    df_merged = pd.merge(products, df, left_on='sku', right_on='Артикул')\n",
    "    df_merged = df_merged[['Артикул', 'Наименование', 'lvl_1','lvl_1_name','lvl_2','lvl_2_name','lvl_3','lvl_3_name','lvl_4','lvl_4_name']]\n",
    "    df_merged = df_merged.drop_duplicates ()\n",
    "    df_merged\n",
    "\n",
    "    if want_to_create == True:\n",
    "        conn = create_engine('postgresql+psycopg2://lesikhina:Xne2J*f~@10.47.126.122:5432/sas')\n",
    "        df_merged.to_sql(name = table_name_for_db, con=conn, schema='public', if_exists='replace')\n",
    "        print('Залито в базу, путь: {}'.format(table_name_for_db))\n",
    "\n",
    "    else:\n",
    "        pass \n",
    "\n",
    "    return df_merged\n",
    "\n",
    "R = Response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        Response\n",
      "\u001b[1;31mString form:\u001b[0m <__main__.Response object at 0x000002AEFAED50C0>\n",
      "\u001b[1;31mDocstring:\u001b[0m  \n",
      "Класс для выгрузки данных с сайта, преобразования и загрузки в Dbeaver.\n",
      "\n",
      "Для проверки подключения использовать - start_respond с аргументом type = test.\n",
      "Для выгрузки основных категорий использовать start_respond с аргументом type = main.\n",
      "\n",
      "Для выгрузки остальных данных использовать метод loader:\n",
      "    links           - Набор ссылок, по которым выгружается информация\n",
      "    type            - (category, uncategory(не обязательно), pages, products, name_art)\n",
      "                        Порядок выгрузки - (category, uncategory(не обязательно), pages, products, name_art)\n",
      "                        При выгрузке pages необходимо указать тип ранее используемого уровня page_type - (category, uncategory).\n",
      "    need_to_red     - (True, False) Дефолтно стоит True. Нужно для преобразования файла в нужный формат. При False выдаст сырой список по товарам с артикулами и наименованиями.\n",
      "\n",
      "Для преобразования данных использовать функцию redactor:\n",
      "    prod_links      - Ссылки продуктов\n",
      "    type            - (category, uncategory). Указать откуда были выгружены ссылки.\n",
      "    want_to_split   - (True, False). Указать True, если хотим разбить df на несколько по категориям. Использовать, если данных очень большое количество.\n",
      "\n",
      "Для загрузки данных в Dbeaver использовать функцию merge_with_db:\n",
      "    df              - Датафрейм с шага redactor\n",
      "    table_name_for_db Наименование таблицы для загрузки в базу\n",
      "    want_to_create  - (True, False) Дефолтно стоит True. Нужно для загрузки в базу. Если поставить False - загружаться не будет. Выдаст итоговый датафрейм."
     ]
    }
   ],
   "source": [
    "R?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8864f0ef7c4be1930c7ccde75ea456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(height='20px', width='20%'), max=5, style=ProgressStyle(bar_color='#f08080'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROCESS: 5/5  ||  100.0 %  ||  WAITING TIME 0.0 MIN  ||   \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df83019210f54c1e8dc218579139d6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(height='20px', width='20%'), max=10, style=ProgressStyle(bar_color='#f08080…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROCESS: 10/10  ||  100.0 %  ||  WAITING TIME 0.0 MIN  ||   \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109e4e972f2b48308a6d7bf0b34b147f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(height='20px', width='20%'), max=10, style=ProgressStyle(bar_color='#f08080…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PROCESS: 10/10  ||  100.0 %  ||  WAITING TIME 0.0 MIN  ||   \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbead2cee1741c89e097196cc04fa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, layout=Layout(height='20px', width='20%'), max=10, style=ProgressStyle(bar_color='#f08080…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные загружены одним файлом. Создан файл с именем: Full_data\n"
     ]
    }
   ],
   "source": [
    "# ПРИМЕРЫ ЗАПРОСОВ\n",
    "\n",
    "# тест главные категории\n",
    "main_links = R.start_respond(type = 'main')\n",
    "main_links = main_links[0:5]\n",
    "\n",
    "# тест категории\n",
    "cat_links = R.respond(links = main_links, type = 'category')\n",
    "cat_links = cat_links[0:10]\n",
    "\n",
    "# тест подкатегории\n",
    "uncat_links = R.respond(type = 'uncategory', links = cat_links)\n",
    "uncat_links = uncat_links[0:10]\n",
    "\n",
    "# тест номера страниц\n",
    "uncat_page_links = R.respond(type = 'pages', links = uncat_links, page_type = 'uncategory')\n",
    "uncat_page_links = uncat_page_links[0:10]\n",
    "\n",
    "# тест продукты\n",
    "prod_links = R.respond(type = 'products', links = uncat_page_links)\n",
    "prod_links = prod_links[0:10]\n",
    "\n",
    "# тест редактор\n",
    "df = redactor(prod_links = prod_links, type = 'uncategory', want_to_split=False)\n",
    "\n",
    "# тест выгрузки наименований и артикулов\n",
    "table_name_art = R.respond(type = 'name_art', links = list(df.path))\n",
    "\n",
    "# тест редактирования списка ссылок с артикулами и наименованиями\n",
    "df = redactor(prod_links = table_name_art, type = 'uncategory', want_to_split = False)\n",
    "\n",
    "# тест заливки в базу\n",
    "merge_with_db(table_name_art, want_to_create = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
